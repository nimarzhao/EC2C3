\chapter{Further Identification Methods}
    \section{Regression Discontinuity Design (RDD)}
        The method exploits a setup in which treatment is based on another variable being above or below a threshold. We can exploit this to create a situation in which treatment is as good as randomly assigned.

    	In particular, RDD is a powerful tool as it creates beautiful and convincing graphs. Showing the effect of $x$ on $y$ can be difficult to communicate to all audiences due to the numerous considerations in applied econometric analysis. Hence convincing graphs are important to convey and communicate results to others.

        \subsection{Regression}
            Let $x_i$ be the `running variable', and $D_i$ be a dummy variable constructed by
            \begin{align}
                D_i =
                \begin{cases}
                    1 &\text{if }x_{i} \geq \theta   \\
                    0 &\text{if }x_{i} < \theta
                \end{cases}
            \end{align}
            where $\theta$ is the threshold.

            Examples of this could be: scholarships awarded if test scores $\geq d$; parliamentary majorities, such as seats $\geq 326$ in the UK House of Commons.

            We want to know the effect of being over the threshold, i.e. $D_i$ on $y_i$. Consider the regression,
            \begin{align}
                y_i = \beta_0 +\beta_1 (x_i-\theta) + \beta_2 D_i + \beta_3 (x_i-\theta)\cdot D_i+u_i \label{eq:RDD/RDD}
            \end{align}
            The regression model in \eqref{eq:RDD/RDD} is plotted in Figure \ref{fig:RDD/RDD}.
            \begin{figure}[h]
                \centering
                \input{figures/RDD}
                \caption{Regression Discontinuity Design (RDD)}
                \label{fig:RDD/RDD}
            \end{figure}

        \subsection{Interpretation}
            How can we interpret the coefficients? To see the meaning behind the regression \eqref{eq:RDD/RDD}. we consider the position and slope separately.\\
            
            \noindent\textbf{Intercept and jump.}
            \begin{itemize}
                \item If we are \textit{exactly at the threshold}, that is $x_i = \theta \implies D_i = 1$, then
                \begin{align}
                    \expect{y_i | x_i = \theta} = \beta_0 + \beta_2
                \end{align}
                
                \item If we are \textit{slightly below the threshold}, $x_i = \theta -\varepsilon$ for a very small $\varepsilon>0$, i.e. very close but not equal to $\theta$
                \begin{align}
                    \expect{y_i\lvert x_i = \theta - \varepsilon} \approx \beta_0
                \end{align}
            \end{itemize}
            \noindent\textbf{Slopes.}
            \begin{itemize}
                \item If we are \textit{below the threshold}, that is $x_i < \theta\implies D_i=0$, then
                \begin{align}
                    \expect{y_i | x_i < \theta} = \beta_0 + \beta_1 (x_i-\theta)
                \end{align}
                and the slope is
                \begin{align}
                    \pdv*{\expect{y_i|x_i<\theta}}{x_i}= \beta_1
                \end{align}

                \item If we are \textit{above the threshold}, that is $x_i\geq \theta\implies D_i = 1$, then
                \begin{align}
                    \expect{y_i | x_i \geq\theta} = \beta_0 + \beta_1 (x_i-\theta) + \beta_2 + \beta_3 (x_i-\theta)
                \end{align}
                and the slope is
                \begin{align}
                    \pdv*{\expect{y_i|x_i\geq\theta}}{x_i}= \beta_1 + \beta_3
                \end{align}
            \end{itemize}


        \subsection{Motivation and assumptions}
            Let us consider an example. Let $x_i$ be `test scores'`, $D_i$ be a scholarship award and $y_i$ be `years of college received'. We want to know the effect receiving a scholarship may have on years of college completed. A `naive' regression would be
            \begin{align}
                y_i = \beta_0 +\beta_1 x_i +\beta_2 D_i + u_i
            \end{align}
            However, this naive regression can have problems. There are many unobserved characteristics (confounders), such as parent’s income, which may simultaneously affect both $x_i$, test scores and $y_i$, years of college.

            Alternatively, we can use our RDD regression which was
            \begin{align}
                y_i = \beta_0 + \beta_1 (x_i-\theta) + \beta_2 D_i + \beta_3 (x_i-\theta)\cdot D_i + u_i
            \end{align}
            With RDD, we need to restrict our observations to people with similar scores around a window centred at $\theta$. We assume unobserved characteristics of individuals who are slightly above the cutoff, $x_i = \theta + \varepsilon$ are likely to be comparable to unobserved characteristics of individuals slightly below the cutoff, $x_i = \theta - \varepsilon$. In other words, we assume \textbf{confounders remain constant slightly above and below the cutoff}, removing selection bias (see chapter on matching). Since confounders are constant, there is no omitted variable bias and we can interpret the slope statistics as causal.

        \subsection{Implementation}
            The steps to implement RDD involve:
            \begin{enumerate}[(1)]
                \item centering the running variable, i.e. create $x_i-\theta$, 
                \item creating a dummy for the cutoff,
                \item creating the interaction term,
                \item performing the regression.
            \end{enumerate}
            The STATA code to perform steps (1) to (4) is in Listing \ref{lst:RDD/regression}.
            \begin{sexylisting}[colback=white, label=lst:RDD/regression]{RDD}
//  (1) Center the running variable,
    gen x_centered = x - cutoff
    
//  (2) Create a dummy for the cutoff,
    gen treat = x >= cutoff
    
//  (3) Create the interaction term,
    gen interaction = treat * x_centered

//  (4) Run the regression:
    reg y x_centred treat interaction, robust, /*
    */  if inrange(x, min, max)
            \end{sexylisting}

        \subsection{Testing assumptions}
            To summarise what we previously mentioned on assumptions, as we restrict values are close to the cutoff, we assume that confounders between the two groups are constant.

            Importantly, for the RDD assumption to hold true, \textbf{individuals must not be able to choose which side of the cutoff they are on}, and so treatment is as good as random around the cutoff. This is another reason why we only use data around a window of the cutoff to make this assumption believable.

            \begin{example}
                Consider the case that if high school students are awarded `honours' certificate if they read 20 books. Students have the choice if they want to read twenty or more books, and many will read just enough to surpass the cutoff. Therefore treatment will not be as good as randomly assigned at the cutoff.
            \end{example}
            How can we `test' the assumptions? There are two methods to do this:
            \begin{enumerate}
                \item \textbf{Looking at density} - if there are more individuals above than below the cutoff, it is likely that individuals are not as good as randomly assigned, as they are choosing to be one side of the cutoff.
                \item \textbf{Looking at covariate values} - if individuals above and below the cutoff have similar values for observable covariates, then individuals above and below look similar, and treatment is more likely to be as good as random.
            \end{enumerate}
            To implement this in STATA, refer to Listing \ref{lst:RDD/test_assumption}.
            \begin{sexylisting}[colback=white, label=lst:RDD/test_assumption]{Testing assumptions}
//  (1) Summarise the data above the cutoff. 
//  List controls w_1, w_2 w.r.t. condition: x >= cutoff
    sum w_1 w_2 if x >= cutoff & inrange(x, min, max)
    
//  (2) Summarise the data below the cutoff.
    sum w_1 w_2 if x < cutoff & inrange(x, min, max)

//  For (1), look if the # of observations, Obs, is similar
//  in both tables. For (2), look to see if covariates are
//  similar in value in both tables.
            \end{sexylisting}

        \subsection{Window choice}
            With RDD we have a choice of bandwidth/window. As discussed in before, the small bandwidth helps support our assumption of as good as random assignment near to cutoff. However this reduces the number of observations. Therefore we face a trade-off when choosing our bandwidth size, denoted by \verb|(min, max)| in the code sections before:
            \begin{itemize}
                \item A \textbf{smaller bandwidth} will reduce the number observations, and decrease the precision of our estimates. However, our assumption is more plausible and bias is reduced.
                \item A \textbf{larger bandwidth} will increase the number observations, and increase the precision of our estimates. However, our assumption is less plausible and bias is increased.
            \end{itemize}
            One suggested method to balance the trade-off is to use `Silverman’s rule of thumb', which chooses the bandwidth
            \begin{align}
                \text{Silverman's bandwidth} = 1.06 \hat \sigma_x n^{-\frac{1}{5}}
            \end{align}
            This is derived by minimising mean squared error (MSE), which as mentioned in ST102 is $(\expect{\hat\theta} -\theta )^2 + \var{\hat\theta}$; although we will not prove it here.

            To implement this, (1) we find the standard deviation of the running variable and the number of observations, (2) calculate Silverman’s bandwidth, and (3) run the regression with Silverman’s bandwidth. In STATA, the code is detailed in Listing \ref{lst:RDD/silverman}.
            \begin{sexylisting}[colback=white, label=lst:RDD/silverman]{Silverman's bandwidth}
//  (1) Output a detailed summary
    sum x, d

//  (2) Calculate the Silverman bandwidth
    gen silverman = 1.06 * r(N)^(-1/5) * r(sd) 
    
//  (3) Run the regression with Silverman's bandwidth
    reg y x_centered treat interaction, robust, /*
    */ if inrange(x, cutoff - silverman, cutoff + silverman) 
            \end{sexylisting}
            In applied research, we use a variety of bandwidths to show that results do not depend greatly on bandwidth choice. If it turns out our bandwidth choice changes our results greatly, then the credibility of results is badly harmed.

        \subsection{Centering}
            We specified the RDD model with a centered $x$ around the cutoff $\theta$. What if we don’t centre on $\theta$, and instead run
            \begin{align}
                y_i = \gamma_0 + \gamma_1 x_i + \gamma_2 D_i + \gamma_3 (x_i \cdot D_i) + u_i
            \end{align}
            Nothing fundamentally changes, only the interpretations of the coefficients when looking at the jump due to $D_i$. To see this, opening the original specification yields
            \begin{align}
                y_i = \beta_0 - \beta_1 \theta + \beta_1 x_i + (\beta_2 - \beta_3\theta) D_i + \beta_3 (x_i \cdot D_i) + u_i
            \end{align}
            From here one can clearly see the correspondence between the two models.

        \subsection{Non-linear RDD}
            If we plot the data and observe we have a non-linear relationship between $y_i$ and $x_i$, for example a quadratic relation, then we can use a quadratic RDD model:
            \begin{align}
                y_i = \beta_0 +\beta_1 (x_i- \theta) + \beta_2 (x_i-\theta)^2+ \beta_3 D_i + \beta_4 (x_i-\theta)\cdot D_i+\beta_5 (x_i - \theta)^2\cdot D_i + u_i
            \end{align}
            Note that with a small window, every polynomial, by Taylor’s theorem is approximately linear. 
	        
            An example in STATA building on preivous listings in this chapter is presented below in \ref{lst:RDD/non_linear}
            
            \begin{sexylisting}[colback=white, label=lst:RDD/non_linear]{Non-linear RDD}
//  (1) Calculate Silverman's bandwidth
    sum x, d
    gen silverman = 1.06*r(N)^(-1/5)*r(sd)

//  (2) Generate linear, quadratic and interaction terms
    gen x_centered = x -cutoff
    gen x_centered2 = x_centered^2
    gen treat = x >= cutoff
    gen interaction = treat * x_centered
    gen interaction2 = treat * x_centered2
    
//  (3) Run the regression with Silverman's bandwidth and all
//	interactions
    reg y x_centered x_centered2 treat interaction interaction2,/*
    /* robust, if inrange(x, cutoff-silverman , cutoff+silverman) 
            \end{sexylisting}
		
		\subsection{Evaluation and summary}
			We have encountered three types of errors:
			\begin{enumerate}[(1)]
				\item measurement error in $x$
				\item omitted confounders
				\item reverse causality
			\end{enumerate}
			RDD helps us to solve (2) bias due to confounders due to the window assumption. However note (1) and (3) are not solved by RDD.
			
			\textbf{RDD is viewed as a very reliable estimation strategy} if you can justify the assumption. Because it is relatively “easy” to check the assumption, RDD is one of the best econometric tools, if you can find a setting to use it. On the other hand, IV is viewed with more caution because the exogeneity assumption is often difficult to believe. DiD is viewed with more caution because parallel trends is tenuous.

